{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to scrape text from a website\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Extract text from website (customize this based on the structure of the website)\n",
        "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "    return text\n",
        "\n",
        "# Function to calculate plagiarism score\n",
        "def calculate_plagiarism_score(written_text, internet_text):\n",
        "    # Preprocess text\n",
        "    written_text_processed = preprocess_text(written_text)\n",
        "    internet_text_processed = preprocess_text(internet_text)\n",
        "\n",
        "    # Feature extraction\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform([written_text_processed, internet_text_processed])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_matrix = cosine_similarity(X)\n",
        "    plagiarism_score = similarity_matrix[0][1]  # Similarity between written text and internet text\n",
        "\n",
        "    return plagiarism_score\n",
        "\n",
        "# Sample written text\n",
        "written_text = input(\"Enter the written text: \")\n",
        "\n",
        "# Sample website URL\n",
        "website_url = input(\"Enter the URL of the website: \")\n",
        "\n",
        "# Scraping text from the website\n",
        "internet_text = scrape_website(website_url)\n",
        "\n",
        "# Calculate plagiarism score\n",
        "plagiarism_score = calculate_plagiarism_score(written_text, internet_text)\n",
        "\n",
        "print(\"Plagiarism Score:\", plagiarism_score)\n",
        "\n",
        "#in general if the score is more than 0.5, then it is considered plagarism whereas if the score is below 0.5 then it is not considered plagarism\n",
        "#in the below example we took the text from a different website and checked it for plagarism. As expected we got a score below 0.5 which is expected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhY83okWR64N",
        "outputId": "5354c654-2984-439f-af07-fffaca997443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the written text: import requests from bs4 import BeautifulSoup from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from nltk.stem import PorterStemmer import string  # Function to preprocess text def preprocess_text(text):     # Tokenize the text     tokens = word_tokenize(text)      # Convert to lowercase     tokens = [token.lower() for token in tokens]      # Remove punctuation     tokens = [token for token in tokens if token not in string.punctuation]      # Remove stopwords     stop_words = set(stopwords.words('english'))     tokens = [token for token in tokens if token not in stop_words]      # Stemming     stemmer = PorterStemmer()     tokens = [stemmer.stem(token) for token in tokens]      return ' '.join(tokens)  # Function to scrape text from a website def scrape_website(url):     response = requests.get(url)     soup = BeautifulSoup(response.text, 'html.parser')     # Extract text from website (customize this based on the structure of the website)     text = ' '.join([p.get_text() for p in soup.find_all('p')])     return text  # Function to calculate plagiarism score def calculate_plagiarism_score(written_text, internet_text):     # Preprocess text     written_text_processed = preprocess_text(written_text)     internet_text_processed = preprocess_text(internet_text)      # Feature extraction     vectorizer = TfidfVectorizer()     X = vectorizer.fit_transform([written_text_processed, internet_text_processed])      # Calculate cosine similarity     similarity_matrix = cosine_similarity(X)     plagiarism_score = similarity_matrix[0][1]  # Similarity between written text and internet text      return plagiarism_score  # Sample written text written_text = input(\"Enter the written text: \")  # Sample website URL website_url = input(\"Enter the URL of the website: \")  # Scraping text from the website internet_text = scrape_website(website_url)  # Calculate plagiarism score plagiarism_score = calculate_plagiarism_score(written_text, internet_text)  print(\"Plagiarism Score:\", plagiarism_score)\n",
            "Enter the URL of the website: https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/\n",
            "Plagiarism Score: 0.041287789106971826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIhBovJnHWll",
        "outputId": "78dacfc0-8a5c-479f-bfb2-e0db03a11a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz0cTS_N813y",
        "outputId": "27b4def9-5f51-46f9-a279-6601217cefff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "K1ds4_Jl9Grr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd5eec7-da22-440b-f27c-980ba5b64580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y07WRvg7fLQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}